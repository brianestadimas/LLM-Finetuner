 * Serving Flask app 'app'
 * Debug mode: on
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://163.180.179.165:5000
[33mPress CTRL+C to quit[0m
 * Restarting with watchdog (inotify)
 * Debugger is active!
 * Debugger PIN: 648-462-265
127.0.0.1 - - [04/Feb/2025 11:28:26] "POST /run_model_llm HTTP/1.1" 200 -
==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.2.
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.96it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.76it/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 291.57 examples/s]
ERROR during finetuning: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
ERROR during finetuning: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Failed to notify API. Status code: 500, Response: {
  "error": "Failed to update status: Attempted to stop pod that does not exist."
}

 * Serving Flask app 'app'
 * Debug mode: on
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://163.180.179.165:5000
[33mPress CTRL+C to quit[0m
 * Restarting with watchdog (inotify)
 * Debugger is active!
 * Debugger PIN: 648-462-265
127.0.0.1 - - [04/Feb/2025 11:31:31] "POST /run_model_llm HTTP/1.1" 200 -
==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.2.
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.677 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.07it/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 276.98 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]  3%|â–Ž         | 1/30 [00:01<00:47,  1.64s/it][1/30, Epoch 1.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.36362838745117, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.0}
  3%|â–Ž         | 1/30 [00:01<00:47,  1.64s/it]  7%|â–‹         | 2/30 [00:01<00:23,  1.22it/s][2/30, Epoch 2.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.27676773071289, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}
  7%|â–‹         | 2/30 [00:01<00:23,  1.22it/s] 10%|â–ˆ         | 3/30 [00:02<00:15,  1.75it/s][3/30, Epoch 3.0] Step	Training Loss: 15.5268
                                              {'loss': 15.5268, 'grad_norm': 44.797119140625, 'learning_rate': 5e-05, 'epoch': 3.0}
 10%|â–ˆ         | 3/30 [00:02<00:15,  1.75it/s] 13%|â–ˆâ–Ž        | 4/30 [00:02<00:11,  2.19it/s][4/30, Epoch 4.0] Step	Training Loss: 15.0981
                                              {'loss': 15.0981, 'grad_norm': 48.02593231201172, 'learning_rate': 4.814814814814815e-05, 'epoch': 4.0}
 13%|â–ˆâ–Ž        | 4/30 [00:02<00:11,  2.19it/s] 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.58it/s][5/30, Epoch 5.0] Step	Training Loss: 13.6837
                                              {'loss': 13.6837, 'grad_norm': 55.241085052490234, 'learning_rate': 4.62962962962963e-05, 'epoch': 5.0}
 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.58it/s] 20%|â–ˆâ–ˆ        | 6/30 [00:02<00:08,  2.85it/s][6/30, Epoch 6.0] Step	Training Loss: 11.8769
                                              {'loss': 11.8769, 'grad_norm': 63.70769119262695, 'learning_rate': 4.4444444444444447e-05, 'epoch': 6.0}
 20%|â–ˆâ–ˆ        | 6/30 [00:02<00:08,  2.85it/s] 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  3.00it/s][7/30, Epoch 7.0] Step	Training Loss: 9.8612
                                              {'loss': 9.8612, 'grad_norm': 61.3924674987793, 'learning_rate': 4.259259259259259e-05, 'epoch': 7.0}
 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  3.00it/s] 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:06,  3.15it/s][8/30, Epoch 8.0] Step	Training Loss: 7.7032
                                              {'loss': 7.7032, 'grad_norm': 41.81081008911133, 'learning_rate': 4.074074074074074e-05, 'epoch': 8.0}
 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:06,  3.15it/s] 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.26it/s][9/30, Epoch 9.0] Step	Training Loss: 6.1682
                                              {'loss': 6.1682, 'grad_norm': 40.187339782714844, 'learning_rate': 3.888888888888889e-05, 'epoch': 9.0}
 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.26it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.27it/s][10/30, Epoch 10.0] Step	Training Loss: 5.1282
                                               {'loss': 5.1282, 'grad_norm': 34.73080062866211, 'learning_rate': 3.7037037037037037e-05, 'epoch': 10.0}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.27it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.25it/s][11/30, Epoch 11.0] Step	Training Loss: 3.7761
                                               {'loss': 3.7761, 'grad_norm': 37.4967041015625, 'learning_rate': 3.518518518518519e-05, 'epoch': 11.0}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.25it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.32it/s][12/30, Epoch 12.0] Step	Training Loss: 2.6146
                                               {'loss': 2.6146, 'grad_norm': 26.025150299072266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 12.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.32it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:05,  3.39it/s][13/30, Epoch 13.0] Step	Training Loss: 2.0798
                                               {'loss': 2.0798, 'grad_norm': 14.87749195098877, 'learning_rate': 3.148148148148148e-05, 'epoch': 13.0}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:05,  3.39it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.39it/s][14/30, Epoch 14.0] Step	Training Loss: 1.9026
                                               {'loss': 1.9026, 'grad_norm': 10.179399490356445, 'learning_rate': 2.962962962962963e-05, 'epoch': 14.0}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.38it/s][15/30, Epoch 15.0] Step	Training Loss: 1.8421
                                               {'loss': 1.8421, 'grad_norm': 7.700695514678955, 'learning_rate': 2.777777777777778e-05, 'epoch': 15.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:04,  3.42it/s][16/30, Epoch 16.0] Step	Training Loss: 1.788
                                               {'loss': 1.788, 'grad_norm': 7.2514238357543945, 'learning_rate': 2.5925925925925925e-05, 'epoch': 16.0}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:04,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.46it/s][17/30, Epoch 17.0] Step	Training Loss: 1.7587
                                               {'loss': 1.7587, 'grad_norm': 7.494617462158203, 'learning_rate': 2.4074074074074074e-05, 'epoch': 17.0}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.46it/s][18/30, Epoch 18.0] Step	Training Loss: 1.7387
                                               {'loss': 1.7387, 'grad_norm': 7.967076301574707, 'learning_rate': 2.2222222222222223e-05, 'epoch': 18.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.46it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.53it/s][19/30, Epoch 19.0] Step	Training Loss: 1.6102
                                               {'loss': 1.6102, 'grad_norm': 4.486743450164795, 'learning_rate': 2.037037037037037e-05, 'epoch': 19.0}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.53it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.53it/s][20/30, Epoch 20.0] Step	Training Loss: 1.5326
                                               {'loss': 1.5326, 'grad_norm': 4.458468437194824, 'learning_rate': 1.8518518518518518e-05, 'epoch': 20.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.50it/s][21/30, Epoch 21.0] Step	Training Loss: 1.5276
                                               {'loss': 1.5276, 'grad_norm': 6.642087459564209, 'learning_rate': 1.6666666666666667e-05, 'epoch': 21.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.50it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.52it/s][22/30, Epoch 22.0] Step	Training Loss: 1.4578
                                               {'loss': 1.4578, 'grad_norm': 3.6447620391845703, 'learning_rate': 1.4814814814814815e-05, 'epoch': 22.0}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:02,  3.43it/s][23/30, Epoch 23.0] Step	Training Loss: 1.3536
                                               {'loss': 1.3536, 'grad_norm': 3.6009573936462402, 'learning_rate': 1.2962962962962962e-05, 'epoch': 23.0}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:02,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.50it/s][24/30, Epoch 24.0] Step	Training Loss: 1.2982
                                               {'loss': 1.2982, 'grad_norm': 3.6184473037719727, 'learning_rate': 1.1111111111111112e-05, 'epoch': 24.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.50it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.50it/s][25/30, Epoch 25.0] Step	Training Loss: 1.283
                                               {'loss': 1.283, 'grad_norm': 3.6477081775665283, 'learning_rate': 9.259259259259259e-06, 'epoch': 25.0}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.50it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.56it/s][26/30, Epoch 26.0] Step	Training Loss: 1.1993
                                               {'loss': 1.1993, 'grad_norm': 3.6786463260650635, 'learning_rate': 7.4074074074074075e-06, 'epoch': 26.0}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.61it/s][27/30, Epoch 27.0] Step	Training Loss: 1.1625
                                               {'loss': 1.1625, 'grad_norm': 3.691460609436035, 'learning_rate': 5.555555555555556e-06, 'epoch': 27.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.61it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.58it/s][28/30, Epoch 28.0] Step	Training Loss: 1.1433
                                               {'loss': 1.1433, 'grad_norm': 3.7092015743255615, 'learning_rate': 3.7037037037037037e-06, 'epoch': 28.0}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.58it/s][29/30, Epoch 29.0] Step	Training Loss: 1.0878
                                               {'loss': 1.0878, 'grad_norm': 3.756436586380005, 'learning_rate': 1.8518518518518519e-06, 'epoch': 29.0}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.57it/s][30/30, Epoch 30.0] Step	Training Loss: 1.0843
                                               {'loss': 1.0843, 'grad_norm': 3.7608132362365723, 'learning_rate': 0.0, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.57it/s][30/30, Epoch 30.0] Step	Training Loss: N/A
                                               {'train_runtime': 11.0094, 'train_samples_per_second': 2.725, 'train_steps_per_second': 2.725, 'train_loss': 4.997932895024618, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  2.73it/s]
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
Failed to notify API. Status code: 500, Response: {
  "error": "Failed to update status: Attempted to stop pod that does not exist."
}

 * Serving Flask app 'app'
 * Debug mode: on
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://163.180.179.165:5000
[33mPress CTRL+C to quit[0m
 * Restarting with watchdog (inotify)
 * Debugger is active!
 * Debugger PIN: 648-462-265
127.0.0.1 - - [04/Feb/2025 11:36:40] "POST /run_model_llm HTTP/1.1" 200 -
==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.2.
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.677 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.09it/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 158.03 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]  3%|â–Ž         | 1/30 [00:01<00:46,  1.60s/it][1/30, Epoch 1.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.36362838745117, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.0}
  3%|â–Ž         | 1/30 [00:01<00:46,  1.60s/it]  7%|â–‹         | 2/30 [00:01<00:23,  1.21it/s][2/30, Epoch 2.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.27676773071289, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}
  7%|â–‹         | 2/30 [00:01<00:23,  1.21it/s] 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s][3/30, Epoch 3.0] Step	Training Loss: 15.5268
                                              {'loss': 15.5268, 'grad_norm': 44.797119140625, 'learning_rate': 5e-05, 'epoch': 3.0}
 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s] 13%|â–ˆâ–Ž        | 4/30 [00:02<00:12,  2.10it/s][4/30, Epoch 4.0] Step	Training Loss: 15.0981
                                              {'loss': 15.0981, 'grad_norm': 48.02593231201172, 'learning_rate': 4.814814814814815e-05, 'epoch': 4.0}
 13%|â–ˆâ–Ž        | 4/30 [00:02<00:12,  2.10it/s] 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.50it/s][5/30, Epoch 5.0] Step	Training Loss: 13.6837
                                              {'loss': 13.6837, 'grad_norm': 55.241085052490234, 'learning_rate': 4.62962962962963e-05, 'epoch': 5.0}
 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.50it/s] 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:08,  2.80it/s][6/30, Epoch 6.0] Step	Training Loss: 11.8769
                                              {'loss': 11.8769, 'grad_norm': 63.70769119262695, 'learning_rate': 4.4444444444444447e-05, 'epoch': 6.0}
 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:08,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  2.90it/s][7/30, Epoch 7.0] Step	Training Loss: 9.8612
                                              {'loss': 9.8612, 'grad_norm': 61.3924674987793, 'learning_rate': 4.259259259259259e-05, 'epoch': 7.0}
 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  2.90it/s] 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:07,  3.02it/s][8/30, Epoch 8.0] Step	Training Loss: 7.7032
                                              {'loss': 7.7032, 'grad_norm': 41.81081008911133, 'learning_rate': 4.074074074074074e-05, 'epoch': 8.0}
 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:07,  3.02it/s] 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.16it/s][9/30, Epoch 9.0] Step	Training Loss: 6.1682
                                              {'loss': 6.1682, 'grad_norm': 40.187339782714844, 'learning_rate': 3.888888888888889e-05, 'epoch': 9.0}
 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.16it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.23it/s][10/30, Epoch 10.0] Step	Training Loss: 5.1282
                                               {'loss': 5.1282, 'grad_norm': 34.73080062866211, 'learning_rate': 3.7037037037037037e-05, 'epoch': 10.0}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.23it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.23it/s][11/30, Epoch 11.0] Step	Training Loss: 3.7761
                                               {'loss': 3.7761, 'grad_norm': 37.4967041015625, 'learning_rate': 3.518518518518519e-05, 'epoch': 11.0}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.23it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.26it/s][12/30, Epoch 12.0] Step	Training Loss: 2.6146
                                               {'loss': 2.6146, 'grad_norm': 26.025150299072266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 12.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.26it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:05,  3.35it/s][13/30, Epoch 13.0] Step	Training Loss: 2.0798
                                               {'loss': 2.0798, 'grad_norm': 14.87749195098877, 'learning_rate': 3.148148148148148e-05, 'epoch': 13.0}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:05,  3.35it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.40it/s][14/30, Epoch 14.0] Step	Training Loss: 1.9026
                                               {'loss': 1.9026, 'grad_norm': 10.179399490356445, 'learning_rate': 2.962962962962963e-05, 'epoch': 14.0}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.40it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.45it/s][15/30, Epoch 15.0] Step	Training Loss: 1.8421
                                               {'loss': 1.8421, 'grad_norm': 7.700695514678955, 'learning_rate': 2.777777777777778e-05, 'epoch': 15.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.45it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.50it/s][16/30, Epoch 16.0] Step	Training Loss: 1.788
                                               {'loss': 1.788, 'grad_norm': 7.2514238357543945, 'learning_rate': 2.5925925925925925e-05, 'epoch': 16.0}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.50it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.52it/s][17/30, Epoch 17.0] Step	Training Loss: 1.7587
                                               {'loss': 1.7587, 'grad_norm': 7.494617462158203, 'learning_rate': 2.4074074074074074e-05, 'epoch': 17.0}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.51it/s][18/30, Epoch 18.0] Step	Training Loss: 1.7387
                                               {'loss': 1.7387, 'grad_norm': 7.967076301574707, 'learning_rate': 2.2222222222222223e-05, 'epoch': 18.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.54it/s][19/30, Epoch 19.0] Step	Training Loss: 1.6102
                                               {'loss': 1.6102, 'grad_norm': 4.486743450164795, 'learning_rate': 2.037037037037037e-05, 'epoch': 19.0}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.54it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.56it/s][20/30, Epoch 20.0] Step	Training Loss: 1.5326
                                               {'loss': 1.5326, 'grad_norm': 4.458468437194824, 'learning_rate': 1.8518518518518518e-05, 'epoch': 20.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.50it/s][21/30, Epoch 21.0] Step	Training Loss: 1.5276
                                               {'loss': 1.5276, 'grad_norm': 6.642087459564209, 'learning_rate': 1.6666666666666667e-05, 'epoch': 21.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.50it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.47it/s][22/30, Epoch 22.0] Step	Training Loss: 1.4578
                                               {'loss': 1.4578, 'grad_norm': 3.6447620391845703, 'learning_rate': 1.4814814814814815e-05, 'epoch': 22.0}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.54it/s][23/30, Epoch 23.0] Step	Training Loss: 1.3536
                                               {'loss': 1.3536, 'grad_norm': 3.6009573936462402, 'learning_rate': 1.2962962962962962e-05, 'epoch': 23.0}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.55it/s][24/30, Epoch 24.0] Step	Training Loss: 1.2982
                                               {'loss': 1.2982, 'grad_norm': 3.6184473037719727, 'learning_rate': 1.1111111111111112e-05, 'epoch': 24.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.55it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.53it/s][25/30, Epoch 25.0] Step	Training Loss: 1.283
                                               {'loss': 1.283, 'grad_norm': 3.6477081775665283, 'learning_rate': 9.259259259259259e-06, 'epoch': 25.0}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.53it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.62it/s][26/30, Epoch 26.0] Step	Training Loss: 1.1993
                                               {'loss': 1.1993, 'grad_norm': 3.6786463260650635, 'learning_rate': 7.4074074074074075e-06, 'epoch': 26.0}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.53it/s][27/30, Epoch 27.0] Step	Training Loss: 1.1625
                                               {'loss': 1.1625, 'grad_norm': 3.691460609436035, 'learning_rate': 5.555555555555556e-06, 'epoch': 27.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.53it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.49it/s][28/30, Epoch 28.0] Step	Training Loss: 1.1433
                                               {'loss': 1.1433, 'grad_norm': 3.7092015743255615, 'learning_rate': 3.7037037037037037e-06, 'epoch': 28.0}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.57it/s][29/30, Epoch 29.0] Step	Training Loss: 1.0878
                                               {'loss': 1.0878, 'grad_norm': 3.756436586380005, 'learning_rate': 1.8518518518518519e-06, 'epoch': 29.0}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.59it/s][30/30, Epoch 30.0] Step	Training Loss: 1.0843
                                               {'loss': 1.0843, 'grad_norm': 3.7608132362365723, 'learning_rate': 0.0, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.59it/s][30/30, Epoch 30.0] Step	Training Loss: N/A
                                               {'train_runtime': 11.0034, 'train_samples_per_second': 2.726, 'train_steps_per_second': 2.726, 'train_loss': 4.997932895024618, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  3.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  2.73it/s]
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
Failed to notify API. Status code: 500, Response: {
  "error": "Failed to update status: Attempted to stop pod that does not exist."
}

 * Serving Flask app 'app'
 * Debug mode: on
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://163.180.179.165:5000
[33mPress CTRL+C to quit[0m
 * Restarting with watchdog (inotify)
 * Debugger is active!
 * Debugger PIN: 648-462-265
127.0.0.1 - - [04/Feb/2025 11:39:50] "POST /run_model_llm HTTP/1.1" 200 -
==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.2.
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.677 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.09it/s]
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 130.51 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]  3%|â–Ž         | 1/30 [00:01<00:47,  1.63s/it][1/30, Epoch 1.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.36362838745117, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.0}
  3%|â–Ž         | 1/30 [00:01<00:47,  1.63s/it]  7%|â–‹         | 2/30 [00:01<00:23,  1.18it/s][2/30, Epoch 2.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.27676773071289, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}
  7%|â–‹         | 2/30 [00:01<00:23,  1.18it/s] 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s][3/30, Epoch 3.0] Step	Training Loss: 15.5268
                                              {'loss': 15.5268, 'grad_norm': 44.797119140625, 'learning_rate': 5e-05, 'epoch': 3.0}
 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s] 13%|â–ˆâ–Ž        | 4/30 [00:02<00:12,  2.06it/s][4/30, Epoch 4.0] Step	Training Loss: 15.0981
                                              {'loss': 15.0981, 'grad_norm': 48.02593231201172, 'learning_rate': 4.814814814814815e-05, 'epoch': 4.0}
 13%|â–ˆâ–Ž        | 4/30 [00:02<00:12,  2.06it/s] 17%|â–ˆâ–‹        | 5/30 [00:02<00:10,  2.42it/s][5/30, Epoch 5.0] Step	Training Loss: 13.6837
                                              {'loss': 13.6837, 'grad_norm': 55.241085052490234, 'learning_rate': 4.62962962962963e-05, 'epoch': 5.0}
 17%|â–ˆâ–‹        | 5/30 [00:02<00:10,  2.42it/s] 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:08,  2.68it/s][6/30, Epoch 6.0] Step	Training Loss: 11.8769
                                              {'loss': 11.8769, 'grad_norm': 63.70769119262695, 'learning_rate': 4.4444444444444447e-05, 'epoch': 6.0}
 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:08,  2.68it/s] 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:08,  2.87it/s][7/30, Epoch 7.0] Step	Training Loss: 9.8612
                                              {'loss': 9.8612, 'grad_norm': 61.3924674987793, 'learning_rate': 4.259259259259259e-05, 'epoch': 7.0}
 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:08,  2.87it/s] 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:07,  3.04it/s][8/30, Epoch 8.0] Step	Training Loss: 7.7032
                                              {'loss': 7.7032, 'grad_norm': 41.81081008911133, 'learning_rate': 4.074074074074074e-05, 'epoch': 8.0}
 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:07,  3.04it/s] 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.19it/s][9/30, Epoch 9.0] Step	Training Loss: 6.1682
                                              {'loss': 6.1682, 'grad_norm': 40.187339782714844, 'learning_rate': 3.888888888888889e-05, 'epoch': 9.0}
 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.19it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.26it/s][10/30, Epoch 10.0] Step	Training Loss: 5.1282
                                               {'loss': 5.1282, 'grad_norm': 34.73080062866211, 'learning_rate': 3.7037037037037037e-05, 'epoch': 10.0}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:06,  3.26it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.35it/s][11/30, Epoch 11.0] Step	Training Loss: 3.7761
                                               {'loss': 3.7761, 'grad_norm': 37.4967041015625, 'learning_rate': 3.518518518518519e-05, 'epoch': 11.0}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.35it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.41it/s][12/30, Epoch 12.0] Step	Training Loss: 2.6146
                                               {'loss': 2.6146, 'grad_norm': 26.025150299072266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 12.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.41it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:04,  3.48it/s][13/30, Epoch 13.0] Step	Training Loss: 2.0798
                                               {'loss': 2.0798, 'grad_norm': 14.87749195098877, 'learning_rate': 3.148148148148148e-05, 'epoch': 13.0}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:05<00:04,  3.48it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.46it/s][14/30, Epoch 14.0] Step	Training Loss: 1.9026
                                               {'loss': 1.9026, 'grad_norm': 10.179399490356445, 'learning_rate': 2.962962962962963e-05, 'epoch': 14.0}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.50it/s][15/30, Epoch 15.0] Step	Training Loss: 1.8421
                                               {'loss': 1.8421, 'grad_norm': 7.700695514678955, 'learning_rate': 2.777777777777778e-05, 'epoch': 15.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.53it/s][16/30, Epoch 16.0] Step	Training Loss: 1.788
                                               {'loss': 1.788, 'grad_norm': 7.2514238357543945, 'learning_rate': 2.5925925925925925e-05, 'epoch': 16.0}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.53it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.54it/s][17/30, Epoch 17.0] Step	Training Loss: 1.7587
                                               {'loss': 1.7587, 'grad_norm': 7.494617462158203, 'learning_rate': 2.4074074074074074e-05, 'epoch': 17.0}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.54it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.53it/s][18/30, Epoch 18.0] Step	Training Loss: 1.7387
                                               {'loss': 1.7387, 'grad_norm': 7.967076301574707, 'learning_rate': 2.2222222222222223e-05, 'epoch': 18.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.54it/s][19/30, Epoch 19.0] Step	Training Loss: 1.6102
                                               {'loss': 1.6102, 'grad_norm': 4.486743450164795, 'learning_rate': 2.037037037037037e-05, 'epoch': 19.0}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.54it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.56it/s][20/30, Epoch 20.0] Step	Training Loss: 1.5326
                                               {'loss': 1.5326, 'grad_norm': 4.458468437194824, 'learning_rate': 1.8518518518518518e-05, 'epoch': 20.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:07<00:02,  3.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.53it/s][21/30, Epoch 21.0] Step	Training Loss: 1.5276
                                               {'loss': 1.5276, 'grad_norm': 6.642087459564209, 'learning_rate': 1.6666666666666667e-05, 'epoch': 21.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.53it/s][22/30, Epoch 22.0] Step	Training Loss: 1.4578
                                               {'loss': 1.4578, 'grad_norm': 3.6447620391845703, 'learning_rate': 1.4814814814814815e-05, 'epoch': 22.0}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.53it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.56it/s][23/30, Epoch 23.0] Step	Training Loss: 1.3536
                                               {'loss': 1.3536, 'grad_norm': 3.6009573936462402, 'learning_rate': 1.2962962962962962e-05, 'epoch': 23.0}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.56it/s][24/30, Epoch 24.0] Step	Training Loss: 1.2982
                                               {'loss': 1.2982, 'grad_norm': 3.6184473037719727, 'learning_rate': 1.1111111111111112e-05, 'epoch': 24.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:08<00:01,  3.56it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.55it/s][25/30, Epoch 25.0] Step	Training Loss: 1.283
                                               {'loss': 1.283, 'grad_norm': 3.6477081775665283, 'learning_rate': 9.259259259259259e-06, 'epoch': 25.0}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.55it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.47it/s][26/30, Epoch 26.0] Step	Training Loss: 1.1993
                                               {'loss': 1.1993, 'grad_norm': 3.6786463260650635, 'learning_rate': 7.4074074074074075e-06, 'epoch': 26.0}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.51it/s][27/30, Epoch 27.0] Step	Training Loss: 1.1625
                                               {'loss': 1.1625, 'grad_norm': 3.691460609436035, 'learning_rate': 5.555555555555556e-06, 'epoch': 27.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:09<00:00,  3.51it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.46it/s][28/30, Epoch 28.0] Step	Training Loss: 1.1433
                                               {'loss': 1.1433, 'grad_norm': 3.7092015743255615, 'learning_rate': 3.7037037037037037e-06, 'epoch': 28.0}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.46it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.38it/s][29/30, Epoch 29.0] Step	Training Loss: 1.0878
                                               {'loss': 1.0878, 'grad_norm': 3.756436586380005, 'learning_rate': 1.8518518518518519e-06, 'epoch': 29.0}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.42it/s][30/30, Epoch 30.0] Step	Training Loss: 1.0843
                                               {'loss': 1.0843, 'grad_norm': 3.7608132362365723, 'learning_rate': 0.0, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.42it/s][30/30, Epoch 30.0] Step	Training Loss: N/A
                                               {'train_runtime': 11.0045, 'train_samples_per_second': 2.726, 'train_steps_per_second': 2.726, 'train_loss': 4.997932895024618, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  3.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:11<00:00,  2.73it/s]
Saving model..
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
Failed to notify API. Status code: 500, Response: {
  "error": "Failed to update status: Attempted to stop pod that does not exist."
}

 * Debug mode: off
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [04/Feb/2025 11:49:30] "POST /run_model_llm HTTP/1.1" 200 -
==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.2.
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.677 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.09it/s]
127.0.0.1 - - [04/Feb/2025 11:50:09] "[31m[1mPOST /run_model_llm HTTP/1.1[0m" 400 -
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 334.45 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]  3%|â–Ž         | 1/30 [00:01<00:45,  1.58s/it][1/30, Epoch 1.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.36362838745117, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.0}
  3%|â–Ž         | 1/30 [00:01<00:45,  1.58s/it]  7%|â–‹         | 2/30 [00:01<00:23,  1.21it/s][2/30, Epoch 2.0] Step	Training Loss: 15.8255
                                              {'loss': 15.8255, 'grad_norm': 45.27676773071289, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}
  7%|â–‹         | 2/30 [00:01<00:23,  1.21it/s] 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s][3/30, Epoch 3.0] Step	Training Loss: 15.5268
                                              {'loss': 15.5268, 'grad_norm': 44.797119140625, 'learning_rate': 5e-05, 'epoch': 3.0}
 10%|â–ˆ         | 3/30 [00:02<00:15,  1.72it/s] 13%|â–ˆâ–Ž        | 4/30 [00:02<00:11,  2.17it/s][4/30, Epoch 4.0] Step	Training Loss: 15.0981
                                              {'loss': 15.0981, 'grad_norm': 48.02593231201172, 'learning_rate': 4.814814814814815e-05, 'epoch': 4.0}
 13%|â–ˆâ–Ž        | 4/30 [00:02<00:11,  2.17it/s] 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.53it/s][5/30, Epoch 5.0] Step	Training Loss: 13.6837
                                              {'loss': 13.6837, 'grad_norm': 55.241085052490234, 'learning_rate': 4.62962962962963e-05, 'epoch': 5.0}
 17%|â–ˆâ–‹        | 5/30 [00:02<00:09,  2.53it/s] 20%|â–ˆâ–ˆ        | 6/30 [00:02<00:08,  2.82it/s][6/30, Epoch 6.0] Step	Training Loss: 11.8769
                                              {'loss': 11.8769, 'grad_norm': 63.70769119262695, 'learning_rate': 4.4444444444444447e-05, 'epoch': 6.0}
 20%|â–ˆâ–ˆ        | 6/30 [00:02<00:08,  2.82it/s] 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  3.04it/s][7/30, Epoch 7.0] Step	Training Loss: 9.8612
                                              {'loss': 9.8612, 'grad_norm': 61.3924674987793, 'learning_rate': 4.259259259259259e-05, 'epoch': 7.0}
 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:07,  3.04it/s] 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:06,  3.23it/s][8/30, Epoch 8.0] Step	Training Loss: 7.7032
                                              {'loss': 7.7032, 'grad_norm': 41.81081008911133, 'learning_rate': 4.074074074074074e-05, 'epoch': 8.0}
 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:03<00:06,  3.23it/s] 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.35it/s][9/30, Epoch 9.0] Step	Training Loss: 6.1682
                                              {'loss': 6.1682, 'grad_norm': 40.187339782714844, 'learning_rate': 3.888888888888889e-05, 'epoch': 9.0}
 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:06,  3.35it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:05,  3.45it/s][10/30, Epoch 10.0] Step	Training Loss: 5.1282
                                               {'loss': 5.1282, 'grad_norm': 34.73080062866211, 'learning_rate': 3.7037037037037037e-05, 'epoch': 10.0}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:04<00:05,  3.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.55it/s][11/30, Epoch 11.0] Step	Training Loss: 3.7761
                                               {'loss': 3.7761, 'grad_norm': 37.4967041015625, 'learning_rate': 3.518518518518519e-05, 'epoch': 11.0}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:04<00:05,  3.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:04,  3.62it/s][12/30, Epoch 12.0] Step	Training Loss: 2.6146
                                               {'loss': 2.6146, 'grad_norm': 26.025150299072266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 12.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:04,  3.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:04<00:04,  3.62it/s][13/30, Epoch 13.0] Step	Training Loss: 2.0798
                                               {'loss': 2.0798, 'grad_norm': 14.87749195098877, 'learning_rate': 3.148148148148148e-05, 'epoch': 13.0}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:04<00:04,  3.62it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.61it/s][14/30, Epoch 14.0] Step	Training Loss: 1.9026
                                               {'loss': 1.9026, 'grad_norm': 10.179399490356445, 'learning_rate': 2.962962962962963e-05, 'epoch': 14.0}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:05<00:04,  3.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.59it/s][15/30, Epoch 15.0] Step	Training Loss: 1.8421
                                               {'loss': 1.8421, 'grad_norm': 7.700695514678955, 'learning_rate': 2.777777777777778e-05, 'epoch': 15.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.60it/s][16/30, Epoch 16.0] Step	Training Loss: 1.788
                                               {'loss': 1.788, 'grad_norm': 7.2514238357543945, 'learning_rate': 2.5925925925925925e-05, 'epoch': 16.0}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:03,  3.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.59it/s][17/30, Epoch 17.0] Step	Training Loss: 1.7587
                                               {'loss': 1.7587, 'grad_norm': 7.494617462158203, 'learning_rate': 2.4074074074074074e-05, 'epoch': 17.0}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:06<00:03,  3.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.60it/s][18/30, Epoch 18.0] Step	Training Loss: 1.7387
                                               {'loss': 1.7387, 'grad_norm': 7.967076301574707, 'learning_rate': 2.2222222222222223e-05, 'epoch': 18.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.60it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.60it/s][19/30, Epoch 19.0] Step	Training Loss: 1.6102
                                               {'loss': 1.6102, 'grad_norm': 4.486743450164795, 'learning_rate': 2.037037037037037e-05, 'epoch': 19.0}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.60it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:06<00:02,  3.63it/s][20/30, Epoch 20.0] Step	Training Loss: 1.5326
                                               {'loss': 1.5326, 'grad_norm': 4.458468437194824, 'learning_rate': 1.8518518518518518e-05, 'epoch': 20.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:06<00:02,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.68it/s][21/30, Epoch 21.0] Step	Training Loss: 1.5276
                                               {'loss': 1.5276, 'grad_norm': 6.642087459564209, 'learning_rate': 1.6666666666666667e-05, 'epoch': 21.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:07<00:02,  3.68it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.67it/s][22/30, Epoch 22.0] Step	Training Loss: 1.4578
                                               {'loss': 1.4578, 'grad_norm': 3.6447620391845703, 'learning_rate': 1.4814814814814815e-05, 'epoch': 22.0}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.68it/s][23/30, Epoch 23.0] Step	Training Loss: 1.3536
                                               {'loss': 1.3536, 'grad_norm': 3.6009573936462402, 'learning_rate': 1.2962962962962962e-05, 'epoch': 23.0}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:01,  3.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:07<00:01,  3.66it/s][24/30, Epoch 24.0] Step	Training Loss: 1.2982
                                               {'loss': 1.2982, 'grad_norm': 3.6184473037719727, 'learning_rate': 1.1111111111111112e-05, 'epoch': 24.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:07<00:01,  3.66it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.66it/s][25/30, Epoch 25.0] Step	Training Loss: 1.283
                                               {'loss': 1.283, 'grad_norm': 3.6477081775665283, 'learning_rate': 9.259259259259259e-06, 'epoch': 25.0}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.65it/s][26/30, Epoch 26.0] Step	Training Loss: 1.1993
                                               {'loss': 1.1993, 'grad_norm': 3.6786463260650635, 'learning_rate': 7.4074074074074075e-06, 'epoch': 26.0}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:08<00:00,  3.65it/s][27/30, Epoch 27.0] Step	Training Loss: 1.1625
                                               {'loss': 1.1625, 'grad_norm': 3.691460609436035, 'learning_rate': 5.555555555555556e-06, 'epoch': 27.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:08<00:00,  3.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.63it/s][28/30, Epoch 28.0] Step	Training Loss: 1.1433
                                               {'loss': 1.1433, 'grad_norm': 3.7092015743255615, 'learning_rate': 3.7037037037037037e-06, 'epoch': 28.0}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:09<00:00,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.64it/s][29/30, Epoch 29.0] Step	Training Loss: 1.0878
                                               {'loss': 1.0878, 'grad_norm': 3.756436586380005, 'learning_rate': 1.8518518518518519e-06, 'epoch': 29.0}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.67it/s][30/30, Epoch 30.0] Step	Training Loss: 1.0843
                                               {'loss': 1.0843, 'grad_norm': 3.7608132362365723, 'learning_rate': 0.0, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.67it/s][30/30, Epoch 30.0] Step	Training Loss: N/A
                                               {'train_runtime': 10.4341, 'train_samples_per_second': 2.875, 'train_steps_per_second': 2.875, 'train_loss': 4.997932895024618, 'epoch': 30.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:10<00:00,  3.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:10<00:00,  2.88it/s]
Saving model..
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
ERROR during finetuning: cannot access local variable 'model' where it is not associated with a value
Failed to notify API. Status code: 500, Response: {
  "error": "Failed to update status: Attempted to stop pod that does not exist."
}

Exception ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>
Traceback (most recent call last):
  File "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/weakref.py", line 666, in _exitfunc
    f()
  File "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/weakref.py", line 590, in __call__
    return info.func(*info.args, **(info.kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/library.py", line 402, in _del_library
    handle.destroy()
KeyboardInterrupt: 
